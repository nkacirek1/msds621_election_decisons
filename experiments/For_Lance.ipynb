{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    \"\"\"\n",
    "    Uses LabelEncoder to encode disrecete values\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(data_f):\n",
    "    \"\"\"\n",
    "    Oversamples the highly contested races\n",
    "    \"\"\"\n",
    "    df_contested = data_f[data_f.TIGHT_RACE == True].reset_index(drop=True)\n",
    "    df_not_contested = data_f[data_f.TIGHT_RACE == False].reset_index(drop=True)\n",
    "    sample = np.random.choice(range(df_contested.shape[0]), size=df_not_contested.shape[0], replace=True)\n",
    "    df_contested_bootstrapped = df_contested.iloc[sample].reset_index(drop=True)\n",
    "    frames = [df_not_contested, df_contested_bootstrapped]\n",
    "    df_new = pd.concat(frames)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- **r100** : 100% of precincts reporting\n",
    "- **r5**: 5% of precincts reporting\n",
    "- **_3**: 3 strata\n",
    "- **_6**: 6 strata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 % reporting\n",
    "r100_3 = pd.read_csv(\"../full_data/final.csv\")\n",
    "r100_6 = pd.read_csv(\"../full_data/full_final.csv\")\n",
    "\n",
    "# 90 % reporting\n",
    "r90_3 = pd.read_csv(\"../partial_data/90/90_reporting.csv\")\n",
    "r90_6 = pd.read_csv(\"../partial_data/90/90_reporting_6.csv\")\n",
    "\n",
    "# 75 % reporting\n",
    "r75_3 = pd.read_csv(\"../partial_data/75/75_reporting.csv\")\n",
    "r75_6 = pd.read_csv(\"../partial_data/75/75_reporting_6.csv\")\n",
    "\n",
    "# 50 % reporting\n",
    "r50_3 = pd.read_csv(\"../partial_data/50/50_reporting.csv\")\n",
    "r50_6 = pd.read_csv(\"../partial_data/50/50_reporting_6.csv\")\n",
    "\n",
    "# 25 % reporting\n",
    "r25_3 = pd.read_csv(\"../partial_data/25/25_reporting.csv\")\n",
    "r25_6 = pd.read_csv(\"../partial_data/25/25_reporting_6.csv\")\n",
    "\n",
    "# 15 % reporting\n",
    "r15_3 = pd.read_csv(\"../partial_data/15/15_reporting.csv\")\n",
    "r15_6 = pd.read_csv(\"../partial_data/15/15_reporting_6.csv\")\n",
    "\n",
    "# 10 % reporting\n",
    "r10_3 = pd.read_csv(\"../partial_data/10/10_reporting.csv\")\n",
    "r10_6 = pd.read_csv(\"../partial_data/10/10_reporting_6.csv\")\n",
    "\n",
    "# 5 % reporting\n",
    "r5_3 = pd.read_csv(\"../partial_data/5/5_reporting.csv\")\n",
    "r5_6 = pd.read_csv(\"../partial_data/5/5_reporting_6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [r100_3, r100_6, r90_3, r90_6, r75_3, r75_6, r50_3, r50_6, \n",
    "               r25_3, r25_6, r15_3, r15_6, r10_3, r10_6, r5_3, r5_6]\n",
    "\n",
    "for d in data_frames:\n",
    "    d = encode(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "leave_one_out = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master list for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plots = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the correlation\n",
    "r100_3_nc = r100_3[['S1_DEM_RATIO','S2_DEM_RATIO', 'S3_DEM_RATIO',\n",
    "                    'S1_REP_RATIO', 'S2_REP_RATIO', 'S3_REP_RATIO',\n",
    "                    'TIGHT_RACE', 'WINNER']]\n",
    "percents = [90, 75, 50, 25, 15, 10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training - seperate the target (WINNER)\n",
    "X_ = r100_3_nc.drop(columns=['WINNER'])\n",
    "y_ = r100_3_nc.WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dfs = [r90_3, r75_3, r50_3, r25_3, r15_3, r10_3, r5_3]\n",
    "testing_nc_dfs = [df[['S1_DEM_RATIO','S2_DEM_RATIO', 'S3_DEM_RATIO',\n",
    "                         'S1_REP_RATIO', 'S2_REP_RATIO', 'S3_REP_RATIO',\n",
    "                         'TIGHT_RACE', 'WINNER']] for df in testing_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(partial_reporting_df, perc):\n",
    "    scores = []\n",
    "    tight_from_test = []\n",
    "    p_scores = []\n",
    "    p_tight = []\n",
    "    \n",
    "    p_X = partial_reporting_df.drop(columns=['WINNER'])\n",
    "    p_y = partial_reporting_df.WINNER\n",
    "    p_tight_only = partial_reporting_df[partial_reporting_df.TIGHT_RACE]\n",
    "    p_tight_X = p_tight_only.drop(columns=['WINNER'])\n",
    "    p_tight_y = p_tight_only.WINNER\n",
    "    \n",
    "    for train_index, test_index in leave_one_out.split(X_):\n",
    "        X_test = X_.iloc[test_index]\n",
    "        y_test = y_.iloc[test_index]\n",
    "\n",
    "        oversampled_df = oversample(r100_3_nc.iloc[train_index])\n",
    "        X_train = oversampled_df.drop(columns=['WINNER'])\n",
    "        y_train = oversampled_df.WINNER\n",
    "\n",
    "        dt = DecisionTreeClassifier(max_depth=6)\n",
    "        dt.fit(X_train, y_train)\n",
    "\n",
    "        scores.append(dt.score(X_test, y_test))\n",
    "        if X_test.TIGHT_RACE.bool():\n",
    "            tight_from_test.append(dt.score(X_test, y_test))\n",
    "        p_scores.append(dt.score(p_X, p_y))\n",
    "        p_tight.append(dt.score(p_tight_X, p_tight_y))\n",
    "\n",
    "    print(np.mean(scores), np.mean(tight_from_test), np.mean(p_scores), np.mean(p_tight), perc)\n",
    "    return[scores, tight_from_test, p_scores, p_tight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125 0.5909090909090909 0.8541238584474886 0.7890809692671393 90\n",
      "0.8229166666666666 0.6363636363636364 0.8454147640791475 0.7751436781609197 75\n",
      "0.7708333333333334 0.6363636363636364 0.853595890410959 0.7910539215686274 50\n",
      "0.7604166666666666 0.7272727272727273 0.853950403690888 0.798068576388889 25\n",
      "0.7708333333333334 0.6363636363636364 0.859965341255332 0.7992424242424242 15\n",
      "0.7708333333333334 0.6363636363636364 0.8388032724505327 0.7466032608695653 10\n",
      "0.75 0.6363636363636364 0.8529757993867717 0.79173801369863 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(percents)):\n",
    "    #decision_tree(testing_nc_dfs[i], percents[i])\n",
    "    for_plots.append(decision_tree(testing_nc_dfs[i], percents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "r100_6_nc = r100_6[['S1_DEM_RATIO','S2_DEM_RATIO', 'S3_DEM_RATIO',\n",
    "                         'S4_DEM_RATIO','S5_DEM_RATIO', 'S6_DEM_RATIO',\n",
    "                         'S1_REP_RATIO', 'S2_REP_RATIO', 'S3_REP_RATIO',\n",
    "                         'S4_REP_RATIO', 'S5_REP_RATIO', 'S6_REP_RATIO',\n",
    "                         'TIGHT_RACE', 'WINNER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dfs = [r90_6, r75_6, r50_6, r25_6, r15_6, r10_6, r5_6]\n",
    "testing_nc_dfs = [df[['S1_DEM_RATIO','S2_DEM_RATIO', 'S3_DEM_RATIO',\n",
    "                         'S4_DEM_RATIO','S5_DEM_RATIO', 'S6_DEM_RATIO',\n",
    "                         'S1_REP_RATIO', 'S2_REP_RATIO', 'S3_REP_RATIO',\n",
    "                         'S4_REP_RATIO', 'S5_REP_RATIO', 'S6_REP_RATIO',\n",
    "                         'TIGHT_RACE', 'WINNER']] for df in testing_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training - seperate the target (WINNER)\n",
    "X_ = r100_6_nc.drop(columns=['WINNER'])\n",
    "y_ = r100_6_nc.WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(partial_reporting_df, perc):\n",
    "    scores = []\n",
    "    tight_from_test = []\n",
    "    p_scores = []\n",
    "    p_tight = []\n",
    "    \n",
    "    p_X = partial_reporting_df.drop(columns=['WINNER'])\n",
    "    p_y = partial_reporting_df.WINNER\n",
    "    p_tight_only = partial_reporting_df[partial_reporting_df.TIGHT_RACE]\n",
    "    p_tight_X = p_tight_only.drop(columns=['WINNER'])\n",
    "    p_tight_y = p_tight_only.WINNER\n",
    "    \n",
    "    for train_index, test_index in leave_one_out.split(X_):\n",
    "        X_test = X_.iloc[test_index]\n",
    "        y_test = y_.iloc[test_index]\n",
    "\n",
    "        oversampled_df = oversample(r100_6_nc.iloc[train_index])\n",
    "        X_train = oversampled_df.drop(columns=['WINNER'])\n",
    "        y_train = oversampled_df.WINNER\n",
    "\n",
    "        rf = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        scores.append(rf.score(X_test, y_test))\n",
    "        if X_test.TIGHT_RACE.bool():\n",
    "            tight_from_test.append(rf.score(X_test, y_test))\n",
    "        p_scores.append(rf.score(p_X, p_y))\n",
    "        p_tight.append(rf.score(p_tight_X, p_tight_y))\n",
    "\n",
    "    print(np.mean(scores), np.mean(tight_from_test), np.mean(p_scores), np.mean(p_tight), perc)\n",
    "    return[scores, tight_from_test, p_scores, p_tight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7916666666666666 0.7727272727272727 0.9122574200913242 0.8503989361702128 90\n",
      "0.8333333333333334 0.8181818181818182 0.9046803652968037 0.8329741379310344 75\n",
      "0.8229166666666666 0.7727272727272727 0.9322203196347031 0.8811274509803924 50\n",
      "0.8229166666666666 0.8181818181818182 0.9028309853353106 0.840576171875 25\n",
      "0.84375 0.7272727272727273 0.9099824801950032 0.8547647527910686 15\n",
      "0.8541666666666666 0.8181818181818182 0.90515601217656 0.8276721014492754 10\n",
      "0.8229166666666666 0.7727272727272727 0.9042925974594832 0.8462828196347032 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(percents)):\n",
    "    #random_forest(testing_nc_dfs[i], percents[i])\n",
    "    for_plots.append(random_forest(testing_nc_dfs[i], percents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boost_tree(partial_reporting_df, perc):\n",
    "    scores = []\n",
    "    tight_from_test = []\n",
    "    p_scores = []\n",
    "    p_tight = []\n",
    "    \n",
    "    p_X = partial_reporting_df.drop(columns=['WINNER'])\n",
    "    p_y = partial_reporting_df.WINNER\n",
    "    p_tight_only = partial_reporting_df[partial_reporting_df.TIGHT_RACE]\n",
    "    p_tight_X = p_tight_only.drop(columns=['WINNER'])\n",
    "    p_tight_y = p_tight_only.WINNER\n",
    "    \n",
    "    for train_index, test_index in leave_one_out.split(X_):\n",
    "        X_test = X_.iloc[test_index]\n",
    "        y_test = y_.iloc[test_index]\n",
    "\n",
    "        oversampled_df = oversample(r100_6_nc.iloc[train_index])\n",
    "        X_train = oversampled_df.drop(columns=['WINNER'])\n",
    "        y_train = oversampled_df.WINNER\n",
    "\n",
    "        gbt = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "        gbt.fit(X_train, y_train)\n",
    "        \n",
    "        scores.append(gbt.score(X_test, y_test))\n",
    "        if X_test.TIGHT_RACE.bool():\n",
    "            tight_from_test.append(gbt.score(X_test, y_test))\n",
    "        p_scores.append(gbt.score(p_X, p_y))\n",
    "        p_tight.append(gbt.score(p_tight_X, p_tight_y))\n",
    "\n",
    "    print(np.mean(scores), np.mean(tight_from_test), np.mean(p_scores), np.mean(p_tight), perc)\n",
    "    return [scores, tight_from_test, p_scores, p_tight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8645833333333334 0.8636363636363636 0.9138984018264841 0.8561613475177307 90\n",
      "0.84375 0.9090909090909091 0.9068207762557078 0.8454262452107278 75\n",
      "0.8854166666666666 0.9545454545454546 0.9443493150684933 0.9362745098039217 50\n",
      "0.8854166666666666 0.9090909090909091 0.9036651425275993 0.8437771267361113 25\n",
      "0.84375 0.9090909090909091 0.9114583333333334 0.8584529505582138 15\n",
      "0.8541666666666666 0.9545454545454546 0.909484398782344 0.8525815217391305 10\n",
      "0.8541666666666666 0.9090909090909091 0.9076325010950503 0.8497431506849314 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(percents)):\n",
    "    #boost_tree(testing_nc_dfs[i], percents[i])\n",
    "    for_plots.append(boost_tree(testing_nc_dfs[i], percents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(for_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
